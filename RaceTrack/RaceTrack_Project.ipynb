{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5dba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "GROUP_ID = \"Group36\"\n",
    "ALGORITHM = \"SARSA\"  # ValItr | QLrng | SARSA\n",
    "TRACK_NAME = \"Tracks/Provided/U-track.txt\"\n",
    "CRASH_POS = \"NRST\"  # NRST | STRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6e1d074-354a-4e11-897d-7845d7a29343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Racing Algorithm Execution\n",
      "Group ID: Group36\n",
      "Algorithm: SARSA\n",
      "Track: Tracks/Provided/2-track.txt\n",
      "Crash Scenario: NRST\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Running SARSA Algorithm\n",
      "Group: Group36\n",
      "Track: Tracks/Provided/2-track.txt\n",
      "Crash Scenario: NRST\n",
      "============================================================\n",
      "Track loaded: 28 x 30\n",
      "Start positions: 5\n",
      "Finish positions: 5\n",
      "Starting SARSA training\n",
      "Episode 100: 0.00\n",
      "Episode 100: 0.00\n",
      "Episode 200: -1.00\n",
      "Episode 200: -1.00\n",
      "Episode 300: 0.00\n",
      "Episode 300: 0.00\n",
      "Episode 400: 0.00\n",
      "Episode 400: 0.00\n",
      "Episode 500: 0.00\n",
      "Episode 500: 0.00\n",
      "Episode 600: 0.00\n",
      "Episode 600: 0.00\n",
      "Episode 700: 0.00\n",
      "Episode 700: 0.00\n",
      "Episode 800: 0.00\n",
      "Episode 800: 0.00\n",
      "Episode 900: 0.00\n",
      "Episode 900: 0.00\n",
      "Episode 1000: 0.00\n",
      "Episode 1000: 0.00\n",
      "Episode 1100: 0.00\n",
      "Episode 1100: 0.00\n",
      "Episode 1200: 0.00\n",
      "Episode 1200: 0.00\n",
      "Episode 1300: 0.00\n",
      "Episode 1300: 0.00\n",
      "Episode 1400: 0.00\n",
      "Episode 1400: 0.00\n",
      "Episode 1500: 0.00\n",
      "Episode 1500: 0.00\n",
      "Episode 1600: 0.00\n",
      "Episode 1600: 0.00\n",
      "Episode 1700: 0.00\n",
      "Episode 1700: 0.00\n",
      "Episode 1800: 0.00\n",
      "Episode 1800: 0.00\n",
      "Episode 1900: -1.00\n",
      "Episode 1900: -1.00\n",
      "Episode 2000: 0.00\n",
      "Episode 2000: 0.00\n",
      "Episode 2100: 0.00\n",
      "Episode 2100: 0.00\n",
      "Episode 2200: 0.00\n",
      "Episode 2200: 0.00\n",
      "Episode 2300: 0.00\n",
      "Episode 2300: 0.00\n",
      "Episode 2400: 0.00\n",
      "Episode 2400: 0.00\n",
      "Episode 2500: 0.00\n",
      "Episode 2500: 0.00\n",
      "Episode 2600: 0.00\n",
      "Episode 2600: 0.00\n",
      "Episode 2700: 0.00\n",
      "Episode 2700: 0.00\n",
      "Episode 2800: 0.00\n",
      "Episode 2800: 0.00\n",
      "Episode 2900: 0.00\n",
      "Episode 2900: 0.00\n",
      "Episode 3000: 0.00\n",
      "Episode 3000: 0.00\n",
      "Episode 3100: 0.00\n",
      "Episode 3100: 0.00\n",
      "Episode 3200: -1.00\n",
      "Episode 3200: -1.00\n",
      "Episode 3300: 0.00\n",
      "Episode 3300: 0.00\n",
      "Episode 3400: 0.00\n",
      "Episode 3400: 0.00\n",
      "Episode 3500: 0.00\n",
      "Episode 3500: 0.00\n",
      "Episode 3600: 0.00\n",
      "Episode 3600: 0.00\n",
      "Episode 3700: 0.00\n",
      "Episode 3700: 0.00\n",
      "Episode 3800: 0.00\n",
      "Episode 3800: 0.00\n",
      "Episode 3900: 0.00\n",
      "Episode 3900: 0.00\n",
      "Episode 4000: 0.00\n",
      "SARSA training complete\n",
      "\n",
      "Running 10 simulations with learned policy...\n",
      "  Simulation 1: Failed to reach finish\n",
      "  Simulation 2: Failed to reach finish\n",
      "  Simulation 3: Success in 39 steps\n",
      "  Simulation 4: Failed to reach finish\n",
      "  Simulation 5: Failed to reach finish\n",
      "  Simulation 6: Failed to reach finish\n",
      "  Simulation 7: Failed to reach finish\n",
      "  Simulation 8: Success in 313 steps\n",
      "  Simulation 9: Success in 130 steps\n",
      "  Simulation 10: Success in 512 steps\n",
      "\n",
      "Simulation Results (SARSA):\n",
      "  Successful runs: 4/10\n",
      "  Average steps: 248.5\n",
      "  Best run: 39 steps\n",
      "Episode 4000: 0.00\n",
      "SARSA training complete\n",
      "\n",
      "Running 10 simulations with learned policy...\n",
      "  Simulation 1: Failed to reach finish\n",
      "  Simulation 2: Failed to reach finish\n",
      "  Simulation 3: Success in 39 steps\n",
      "  Simulation 4: Failed to reach finish\n",
      "  Simulation 5: Failed to reach finish\n",
      "  Simulation 6: Failed to reach finish\n",
      "  Simulation 7: Failed to reach finish\n",
      "  Simulation 8: Success in 313 steps\n",
      "  Simulation 9: Success in 130 steps\n",
      "  Simulation 10: Success in 512 steps\n",
      "\n",
      "Simulation Results (SARSA):\n",
      "  Successful runs: 4/10\n",
      "  Average steps: 248.5\n",
      "  Best run: 39 steps\n",
      "Saved plot to Group36_SARSA_2-track_NRST.png\n",
      "Saved plot to Group36_SARSA_2-track_NRST.png\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the racing algorithms\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nRacing Algorithm Execution\")\n",
    "    print(f\"Group ID: {GROUP_ID}\")\n",
    "    print(f\"Algorithm: {ALGORITHM}\")\n",
    "    print(f\"Track: {TRACK_NAME}\")\n",
    "    print(f\"Crash Scenario: {CRASH_POS}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    # Check which algorithm to run\n",
    "    if ALGORITHM == \"ValItr\":\n",
    "        # Run Value Iteration\n",
    "        racer = run_value_iteration(GROUP_ID, TRACK_NAME, CRASH_POS)\n",
    "        \n",
    "    elif ALGORITHM == \"QLrng\":\n",
    "        # Run Q-Learning\n",
    "        racer = run_q_learning(GROUP_ID, TRACK_NAME, CRASH_POS)\n",
    "        \n",
    "    elif ALGORITHM == \"SARSA\":\n",
    "        # Run SARSA\n",
    "        racer = run_sarsa(GROUP_ID, TRACK_NAME, CRASH_POS)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: Unknown algorithm '{ALGORITHM}'\")\n",
    "        print(\"Valid algorithms are: ValItr, QLrng, SARSA\")\n",
    "        racer = None\n",
    "    \n",
    "    return racer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0584256f-1506-4966-a471-f3737098bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "\n",
    "class Racetrack:\n",
    "    def __init__(self, track_file, crash_scenario=\"NRST\"):\n",
    "\n",
    "        with open(track_file, 'r') as f:\n",
    "            # Read dimensions from first line\n",
    "            dims = f.readline().strip()\n",
    "            self.rows, self.cols = map(int, dims.split(','))\n",
    "            \n",
    "            # Read track\n",
    "            self.track = []\n",
    "            for _ in range(self.rows):\n",
    "                line = f.readline().rstrip('\\n')\n",
    "                # Pad line to ensure it has exactly cols characters\n",
    "                if len(line) < self.cols:\n",
    "                    line = line + ' ' * (self.cols - len(line))\n",
    "                self.track.append(list(line[:self.cols]))\n",
    "        \n",
    "        self.crash_scenario = crash_scenario\n",
    "        \n",
    "        # Find all start and finish positions\n",
    "        self.start_positions = []\n",
    "        self.finish_positions = []\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "                if self.track[r][c] == 'S':\n",
    "                    self.start_positions.append((r, c))\n",
    "                elif self.track[r][c] == 'F':\n",
    "                    self.finish_positions.append((r, c))\n",
    "        \n",
    "        # Check if position is on track\n",
    "        self.track_positions = []\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols):\n",
    "                if self.track[r][c] in ['.', 'S', 'F']:\n",
    "                    self.track_positions.append((r, c))\n",
    "    \n",
    "    def is_on_track(self, position):\n",
    "        \"\"\"Check if a position is on the track\"\"\"\n",
    "        r, c = position\n",
    "        if 0 <= r < self.rows and 0 <= c < self.cols:\n",
    "            return self.track[r][c] in ['.', 'S', 'F']\n",
    "        return False\n",
    "    \n",
    "    def is_finish(self, position):\n",
    "        \"\"\"Check if position is a finish line cell\"\"\"\n",
    "        r, c = position\n",
    "        if 0 <= r < self.rows and 0 <= c < self.cols:\n",
    "            return self.track[r][c] == 'F'\n",
    "        return False\n",
    "    \n",
    "    def get_nearest_track_position(self, position):\n",
    "        \"\"\"Find nearest track position using Manhattan distance\"\"\"\n",
    "        r, c = position\n",
    "        min_dist = float('inf')\n",
    "        nearest = None\n",
    "        \n",
    "        for tr, tc in self.track_positions:\n",
    "            dist = abs(tr - r) + abs(tc - c)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                nearest = (tr, tc)\n",
    "        \n",
    "        return nearest\n",
    "    \n",
    "    def bresenham_line(self, start, end):\n",
    "        \"\"\"\n",
    "        Return all points on the line from start to end using Bresenham's algorithm\n",
    "        \"\"\"\n",
    "        r1, c1 = start\n",
    "        r2, c2 = end\n",
    "        \n",
    "        points = []\n",
    "        \n",
    "        # Check for special cases\n",
    "        if r1 == r2:\n",
    "            # Horizontal line\n",
    "            step = 1 if c2 > c1 else -1\n",
    "            for c in range(c1, c2 + step, step):\n",
    "                points.append((r1, c))\n",
    "            return points\n",
    "        \n",
    "        if c1 == c2:\n",
    "            # Vertical line\n",
    "            step = 1 if r2 > r1 else -1\n",
    "            for r in range(r1, r2 + step, step):\n",
    "                points.append((r, c1))\n",
    "            return points\n",
    "        \n",
    "        # General case\n",
    "        dr = abs(r2 - r1)\n",
    "        dc = abs(c2 - c1)\n",
    "        \n",
    "        if dr > dc:\n",
    "            # More vertical than horizontal\n",
    "            if r1 > r2:\n",
    "                r1, r2 = r2, r1\n",
    "                c1, c2 = c2, c1\n",
    "            dx = c2 - c1\n",
    "            dy = r2 - r1\n",
    "            xi = 1\n",
    "            if dx < 0:\n",
    "                xi = -1\n",
    "                dx = -dx\n",
    "            D = 2*dx - dy\n",
    "            c = c1\n",
    "            \n",
    "            for r in range(r1, r2 + 1):\n",
    "                points.append((r, c))\n",
    "                if D > 0:\n",
    "                    c += xi\n",
    "                    D -= 2*dy\n",
    "                D += 2*dx\n",
    "        else:\n",
    "            # More horizontal than vertical\n",
    "            if c1 > c2:\n",
    "                r1, r2 = r2, r1\n",
    "                c1, c2 = c2, c1\n",
    "            dx = c2 - c1\n",
    "            dy = r2 - r1\n",
    "            yi = 1\n",
    "            if dy < 0:\n",
    "                yi = -1\n",
    "                dy = -dy\n",
    "            D = 2*dy - dx\n",
    "            r = r1\n",
    "            \n",
    "            for c in range(c1, c2 + 1):\n",
    "                points.append((r, c))\n",
    "                if D > 0:\n",
    "                    r += yi\n",
    "                    D -= 2*dx\n",
    "                D += 2*dy\n",
    "        \n",
    "        return points\n",
    "    \n",
    "    def check_crash(self, old_pos, new_pos):\n",
    "        \"\"\"\n",
    "        Check if car crashes between old_pos and new_pos\n",
    "        Returns: (crashed, crash_position)\n",
    "        \"\"\"\n",
    "        line_points = self.bresenham_line(old_pos, new_pos)\n",
    "        \n",
    "        for point in line_points:\n",
    "            r, c = point\n",
    "            if not (0 <= r < self.rows and 0 <= c < self.cols):\n",
    "                return True, point\n",
    "            if self.track[r][c] == '#':\n",
    "                return True, point\n",
    "        \n",
    "        return False, None\n",
    "\n",
    "\n",
    "class ValueIterationRacer:\n",
    "    def __init__(self, racetrack, gamma=0.9, theta=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize Value Iteration algorithm for racetrack\n",
    "        \"\"\"\n",
    "        self.track = racetrack\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        \n",
    "        # Define state space\n",
    "        # State: (row, col, vel_r, vel_c) where velocities are in [-5, 5]\n",
    "        self.velocities = list(range(-5, 6))\n",
    "        self.actions = [(a_r, a_c) for a_r in [-1, 0, 1] for a_c in [-1, 0, 1]]\n",
    "        \n",
    "        # Initialize value function and policy\n",
    "        self.V = defaultdict(float)  # Value function\n",
    "        self.policy = defaultdict(lambda: (0, 0))  # Policy\n",
    "        \n",
    "        # Initialize Q-values\n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        \"\"\"\n",
    "        Get possible next states and probabilities for a given state and action\n",
    "        Returns: list of (next_state, probability, cost) tuples\n",
    "        \"\"\"\n",
    "        r, c, v_r, v_c = state\n",
    "        a_r, a_c = action\n",
    "        \n",
    "        # Try to apply acceleration (80% success rate)\n",
    "        # With 20% probability, acceleration fails\n",
    "        outcomes = []\n",
    "        \n",
    "        # Outcome 1: Acceleration succeeds (80% probability)\n",
    "        new_v_r = max(-5, min(5, v_r + a_r))\n",
    "        new_v_c = max(-5, min(5, v_c + a_c))\n",
    "        new_r = r + new_v_r\n",
    "        new_c = c + new_v_c\n",
    "        \n",
    "        # Check for crash\n",
    "        crashed, crash_pos = self.track.check_crash((r, c), (new_r, new_c))\n",
    "        \n",
    "        if crashed:\n",
    "            if self.track.crash_scenario == \"NRST\":\n",
    "                # Move to nearest track position, velocity set to 0\n",
    "                nearest = self.track.get_nearest_track_position(crash_pos)\n",
    "                next_state = (nearest[0], nearest[1], 0, 0)\n",
    "            else:  # \"STRT\"\n",
    "                # Move to random start position, velocity set to 0\n",
    "                start_pos = random.choice(self.track.start_positions)\n",
    "                next_state = (start_pos[0], start_pos[1], 0, 0)\n",
    "        else:\n",
    "            next_state = (new_r, new_c, new_v_r, new_v_c)\n",
    "        \n",
    "        # Check if next state is finish\n",
    "        if self.track.is_finish((next_state[0], next_state[1])):\n",
    "            cost = 0  # Finish state has 0 cost\n",
    "        else:\n",
    "            cost = 1  # Regular move cost\n",
    "        \n",
    "        outcomes.append((next_state, 0.8, cost))\n",
    "        \n",
    "        # Outcome 2: Acceleration fails (20% probability)\n",
    "        new_v_r = v_r\n",
    "        new_v_c = v_c\n",
    "        new_r = r + new_v_r\n",
    "        new_c = c + new_v_c\n",
    "        \n",
    "        # Check for crash\n",
    "        crashed, crash_pos = self.track.check_crash((r, c), (new_r, new_c))\n",
    "        \n",
    "        if crashed:\n",
    "            if self.track.crash_scenario == \"NRST\":\n",
    "                nearest = self.track.get_nearest_track_position(crash_pos)\n",
    "                next_state = (nearest[0], nearest[1], 0, 0)\n",
    "            else:  # \"STRT\"\n",
    "                start_pos = random.choice(self.track.start_positions)\n",
    "                next_state = (start_pos[0], start_pos[1], 0, 0)\n",
    "        else:\n",
    "            next_state = (new_r, new_c, new_v_r, new_v_c)\n",
    "        \n",
    "        # Check if next state is finish\n",
    "        if self.track.is_finish((next_state[0], next_state[1])):\n",
    "            cost = 0\n",
    "        else:\n",
    "            cost = 1\n",
    "        \n",
    "        outcomes.append((next_state, 0.2, cost))\n",
    "        \n",
    "        return outcomes\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Perform Value Iteration algorithm\n",
    "        \"\"\"\n",
    "        print(\"Starting Value Iteration...\")\n",
    "        \n",
    "        iteration = 0\n",
    "        max_delta = float('inf')\n",
    "        \n",
    "        # Initialize all states\n",
    "        states = []\n",
    "        for r in range(self.track.rows):\n",
    "            for c in range(self.track.cols):\n",
    "                if self.track.is_on_track((r, c)):\n",
    "                    for v_r in self.velocities:\n",
    "                        for v_c in self.velocities:\n",
    "                            states.append((r, c, v_r, v_c))\n",
    "        \n",
    "        while max_delta > self.theta:\n",
    "            max_delta = 0\n",
    "            delta = 0\n",
    "            \n",
    "            for state in states:\n",
    "                r, c, v_r, v_c = state\n",
    "                \n",
    "                # Skip if this is a finish state\n",
    "                if self.track.is_finish((r, c)):\n",
    "                    continue\n",
    "                \n",
    "                # Store old value\n",
    "                old_value = self.V[state]\n",
    "                \n",
    "                # Find best action value\n",
    "                best_value = float('inf')\n",
    "                best_action = None\n",
    "                \n",
    "                for action in self.actions:\n",
    "                    action_value = 0\n",
    "                    outcomes = self.get_next_state(state, action)\n",
    "                    \n",
    "                    for next_state, prob, cost in outcomes:\n",
    "                        # Bellman equation\n",
    "                        action_value += prob * (cost + self.gamma * self.V[next_state])\n",
    "                    \n",
    "                    # Update Q-value\n",
    "                    self.Q[state][action] = action_value\n",
    "                    \n",
    "                    if action_value < best_value:\n",
    "                        best_value = action_value\n",
    "                        best_action = action\n",
    "                \n",
    "                # Update value function\n",
    "                self.V[state] = best_value\n",
    "                self.policy[state] = best_action\n",
    "                \n",
    "                # Update delta\n",
    "                delta = abs(old_value - self.V[state])\n",
    "                if delta > max_delta:\n",
    "                    max_delta = delta\n",
    "            \n",
    "            iteration += 1\n",
    "            if iteration % 10 == 0:\n",
    "                print(f\"Iteration {iteration}, Max Delta: {max_delta:.6f}\")\n",
    "        \n",
    "        print(f\"Value Iteration converged after {iteration} iterations\")\n",
    "        \n",
    "        return self.policy\n",
    "    \n",
    "    def extract_policy(self):\n",
    "        \"\"\"Extract optimal policy from value function\"\"\"\n",
    "        for state in self.V.keys():\n",
    "            best_action = None\n",
    "            best_value = float('inf')\n",
    "            \n",
    "            for action in self.actions:\n",
    "                if self.Q[state][action] < best_value:\n",
    "                    best_value = self.Q[state][action]\n",
    "                    best_action = action\n",
    "            \n",
    "            self.policy[state] = best_action\n",
    "        \n",
    "        return self.policy\n",
    "    \n",
    "    def run_simulation(self, policy=None, max_steps=1000):\n",
    "        \"\"\"\n",
    "        Run a simulation using the learned policy\n",
    "        Returns: (path, steps, success)\n",
    "        \"\"\"\n",
    "        if policy is None:\n",
    "            policy = self.policy\n",
    "        \n",
    "        # Start from a random start position with 0 velocity\n",
    "        start_pos = random.choice(self.track.start_positions)\n",
    "        state = (start_pos[0], start_pos[1], 0, 0)\n",
    "        \n",
    "        path = [state[:2]]  # Store only positions\n",
    "        steps = 0\n",
    "        success = False\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            r, c, v_r, v_c = state\n",
    "            \n",
    "            # Check if we reached finish\n",
    "            if self.track.is_finish((r, c)):\n",
    "                success = True\n",
    "                break\n",
    "            \n",
    "            # Get action from policy\n",
    "            if state in policy:\n",
    "                action = policy[state]\n",
    "            else:\n",
    "                # If state not in policy, choose random action\n",
    "                action = random.choice(self.actions)\n",
    "            \n",
    "            # Simulate with stochastic outcomes\n",
    "            if random.random() < 0.8:\n",
    "                # Acceleration succeeds\n",
    "                a_r, a_c = action\n",
    "                new_v_r = max(-5, min(5, v_r + a_r))\n",
    "                new_v_c = max(-5, min(5, v_c + a_c))\n",
    "            else:\n",
    "                # Acceleration fails\n",
    "                new_v_r = v_r\n",
    "                new_v_c = v_c\n",
    "            \n",
    "            new_r = r + new_v_r\n",
    "            new_c = c + new_v_c\n",
    "            \n",
    "            # Check for crash\n",
    "            crashed, crash_pos = self.track.check_crash((r, c), (new_r, new_c))\n",
    "            \n",
    "            if crashed:\n",
    "                if self.track.crash_scenario == \"NRST\":\n",
    "                    nearest = self.track.get_nearest_track_position(crash_pos)\n",
    "                    state = (nearest[0], nearest[1], 0, 0)\n",
    "                else:  # \"STRT\"\n",
    "                    start_pos = random.choice(self.track.start_positions)\n",
    "                    state = (start_pos[0], start_pos[1], 0, 0)\n",
    "            else:\n",
    "                state = (new_r, new_c, new_v_r, new_v_c)\n",
    "            \n",
    "            path.append(state[:2])\n",
    "            steps += 1\n",
    "        \n",
    "        return path, steps, success\n",
    "    \n",
    "    def plot_path(self, path, group_id, algorithm, track_name, crash_pos):\n",
    "        \"\"\"\n",
    "        Plot the path taken by the agent\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        \n",
    "        # Plot track\n",
    "        track_grid = np.zeros((self.track.rows, self.track.cols))\n",
    "        for r in range(self.track.rows):\n",
    "            for c in range(self.track.cols):\n",
    "                if self.track.track[r][c] == '#':\n",
    "                    track_grid[r, c] = 1  # Wall\n",
    "                elif self.track.track[r][c] == 'S':\n",
    "                    track_grid[r, c] = 2  # Start\n",
    "                elif self.track.track[r][c] == 'F':\n",
    "                    track_grid[r, c] = 3  # Finish\n",
    "                else:\n",
    "                    track_grid[r, c] = 0  # Track\n",
    "        \n",
    "        # Create custom colormap\n",
    "        from matplotlib.colors import ListedColormap\n",
    "        cmap = ListedColormap(['white', 'black', 'green', 'red'])\n",
    "        \n",
    "        ax.imshow(track_grid, cmap=cmap, origin='upper')\n",
    "        \n",
    "        # Plot path\n",
    "        if path:\n",
    "            path_y, path_x = zip(*path)\n",
    "            ax.plot(path_x, path_y, 'b-', linewidth=2, alpha=0.7)\n",
    "            ax.plot(path_x, path_y, 'bo', markersize=4, alpha=0.5)\n",
    "            \n",
    "            # Mark start and end\n",
    "            ax.plot(path_x[0], path_y[0], 'go', markersize=10, label='Start')\n",
    "            if len(path) > 1:\n",
    "                ax.plot(path_x[-1], path_y[-1], 'ro', markersize=10, label='End')\n",
    "        \n",
    "        ax.set_title(f'{algorithm} on {track_name} ({crash_pos} crash scenario)')\n",
    "        ax.set_xlabel('Column')\n",
    "        ax.set_ylabel('Row')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Save figure\n",
    "        filename = f\"{group_id}_{algorithm}_{track_name.replace('.txt', '')}_{crash_pos}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Saved plot to {filename}\")\n",
    "\n",
    "\n",
    "# Base class for all algorithms\n",
    "class BaseRacingAlgorithm:\n",
    "    def __init__(self, racetrack, **kwargs):\n",
    "        self.track = racetrack\n",
    "        self.params = kwargs\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Train the algorithm - to be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def run_simulation(self, max_steps=1000):\n",
    "        \"\"\"Run a simulation - to be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def plot_path(self, path, group_id, algorithm, track_name, crash_pos):\n",
    "        \"\"\"Plot the path - common for all algorithms\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        \n",
    "        # Plot track\n",
    "        track_grid = np.zeros((self.track.rows, self.track.cols))\n",
    "        for r in range(self.track.rows):\n",
    "            for c in range(self.track.cols):\n",
    "                if self.track.track[r][c] == '#':\n",
    "                    track_grid[r, c] = 1  # Wall\n",
    "                elif self.track.track[r][c] == 'S':\n",
    "                    track_grid[r, c] = 2  # Start\n",
    "                elif self.track.track[r][c] == 'F':\n",
    "                    track_grid[r, c] = 3  # Finish\n",
    "                else:\n",
    "                    track_grid[r, c] = 0  # Track\n",
    "        \n",
    "        # Create custom colormap\n",
    "        from matplotlib.colors import ListedColormap\n",
    "        cmap = ListedColormap(['white', 'black', 'green', 'red'])\n",
    "        \n",
    "        ax.imshow(track_grid, cmap=cmap, origin='upper')\n",
    "        \n",
    "        # Plot path\n",
    "        if path:\n",
    "            path_y, path_x = zip(*path)\n",
    "            ax.plot(path_x, path_y, 'b-', linewidth=2, alpha=0.7)\n",
    "            ax.plot(path_x, path_y, 'bo', markersize=4, alpha=0.5)\n",
    "            \n",
    "            # Mark start and end\n",
    "            ax.plot(path_x[0], path_y[0], 'go', markersize=10, label='Start')\n",
    "            if len(path) > 1:\n",
    "                ax.plot(path_x[-1], path_y[-1], 'ro', markersize=10, label='End')\n",
    "        \n",
    "        ax.set_title(f'{algorithm} on {track_name} ({crash_pos} crash scenario)')\n",
    "        ax.set_xlabel('Column')\n",
    "        ax.set_ylabel('Row')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Save figure\n",
    "        filename = f\"{group_id}_{algorithm}_{track_name.replace('.txt', '')}_{crash_pos}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Saved plot to {filename}\")\n",
    "\n",
    "\n",
    "def run_value_iteration(group_id, track_name, crash_pos):\n",
    "    \"\"\"Run Value Iteration algorithm\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running Value Iteration Algorithm\")\n",
    "    print(f\"Group: {group_id}\")\n",
    "    print(f\"Track: {track_name}\")\n",
    "    print(f\"Crash Scenario: {crash_pos}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load racetrack\n",
    "    try:\n",
    "        racetrack = Racetrack(track_name, crash_scenario=crash_pos)\n",
    "        print(f\"Track loaded: {racetrack.rows} x {racetrack.cols}\")\n",
    "        print(f\"Start positions: {len(racetrack.start_positions)}\")\n",
    "        print(f\"Finish positions: {len(racetrack.finish_positions)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Track file '{track_name}' not found.\")\n",
    "        return None\n",
    "    \n",
    "    # Adjust parameters based on crash scenario\n",
    "    if crash_pos == \"STRT\":\n",
    "        gamma = 0.8\n",
    "        theta = 1e-3\n",
    "    else:\n",
    "        gamma = 0.9\n",
    "        theta = 1e-3\n",
    "    \n",
    "    # Create and train Value Iteration racer\n",
    "    vi_racer = ValueIterationRacer(racetrack, gamma=gamma, theta=theta)\n",
    "    policy = vi_racer.train()\n",
    "    \n",
    "    # Run multiple simulations\n",
    "    num_simulations = 10\n",
    "    successful_runs = 0\n",
    "    total_steps = 0\n",
    "    best_path = None\n",
    "    best_steps = float('inf')\n",
    "    \n",
    "    print(f\"\\nRunning {num_simulations} simulations...\")\n",
    "    for i in range(num_simulations):\n",
    "        path, steps, success = vi_racer.run_simulation(policy, max_steps=1000)\n",
    "        \n",
    "        if success:\n",
    "            successful_runs += 1\n",
    "            total_steps += steps\n",
    "            \n",
    "            if steps < best_steps:\n",
    "                best_steps = steps\n",
    "                best_path = path\n",
    "            \n",
    "            print(f\"  Simulation {i+1}: Success in {steps} steps\")\n",
    "        else:\n",
    "            print(f\"  Simulation {i+1}: Failed to reach finish\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nSimulation Results:\")\n",
    "    print(f\"  Successful runs: {successful_runs}/{num_simulations}\")\n",
    "    if successful_runs > 0:\n",
    "        avg_steps = total_steps / successful_runs\n",
    "        print(f\"  Average steps: {avg_steps:.1f}\")\n",
    "        print(f\"  Best run: {best_steps} steps\")\n",
    "        \n",
    "        # Plot best path\n",
    "        track_base = track_name.split('/')[-1].replace('.txt', '')\n",
    "        vi_racer.plot_path(best_path, group_id, \"ValItr\", track_base, crash_pos)\n",
    "    else:\n",
    "        print(\"  No successful runs to plot.\")\n",
    "    \n",
    "    return vi_racer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "469fda02-8ccd-4281-8758-f41d92423432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class QLearningRacer(BaseRacingAlgorithm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        racetrack,\n",
    "        learning_rate=0.1,\n",
    "        discount_factor=0.95,\n",
    "        exploration_rate=0.1,\n",
    "        number_episodes=4000,       #may run slowly but is more reliable on hard tracks\n",
    "        max_steps=1000,\n",
    "    ):\n",
    "        super().__init__(racetrack)     # store racetrack object\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.number_episodes = number_episodes\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        self.actions = []       # all possible accelerations\n",
    "        for acc_row in [-1, 0, 1]:\n",
    "            for acc_col in [-1, 0, 1]:\n",
    "                self.actions.append((acc_row, acc_col))\n",
    "        self.q_table = {}\n",
    "\n",
    "    def get_q_val(self, state, action):     # return Q(s,a) default to 0.0\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "\n",
    "    def set_q_val(self, state, action, val):\n",
    "        self.q_table[(state, action)] = val\n",
    "\n",
    "    def choose_greedy_action(self, state):      # choose action with highest Q-value\n",
    "        best_val = float(\"-inf\")\n",
    "        best_actions = []\n",
    "\n",
    "        for action in self.actions:\n",
    "            q_val = self.get_q_val(state, action)\n",
    "\n",
    "            if q_val > best_val:\n",
    "                best_val = q_val\n",
    "                best_actions = [action]      # create best action list\n",
    "            elif q_val == best_val:\n",
    "                best_actions.append(action)   # tie: add to list\n",
    "\n",
    "        if not best_actions: # fallback to random action\n",
    "            return random.choice(self.actions)\n",
    "\n",
    "        return random.choice(best_actions)      # randomly choose between equally good actions\n",
    "\n",
    "\n",
    "    def choose_action(self, state, use_exploration=True):\n",
    "        if use_exploration and random.random() < self.exploration_rate:\n",
    "            return random.choice(self.actions)      # explore random action\n",
    "        else:\n",
    "            return self.choose_greedy_action(state) # choose best action\n",
    "\n",
    "    def step_env(self, state, action):      # apply one step at a time\n",
    "        pos_row, pos_col, vel_row, vel_col = state\n",
    "        acc_row, acc_col = action\n",
    "\n",
    "        if random.random() < 0.8:   #80% chance of applying acceleration\n",
    "            new_vel_row = min(5, max(-5, vel_row + acc_row))\n",
    "            new_vel_col = min(5, max(-5, vel_col + acc_col))\n",
    "        else:\n",
    "            new_vel_row = vel_row\n",
    "            new_vel_col = vel_col\n",
    "\n",
    "        new_pos_row = pos_row + new_vel_row\n",
    "        new_pos_col = pos_col + new_vel_col\n",
    "\n",
    "        #   crash detection\n",
    "        crashed, crash_pos = self.track.check_crash((pos_row, pos_col), (new_pos_row, new_pos_col))\n",
    "\n",
    "        if crashed:\n",
    "            if self.track.crash_scenario == \"NRST\":\n",
    "                near_row, near_col = self.track.get_nearest_track_position(crash_pos)\n",
    "                next_state = (near_row, near_col, 0, 0)\n",
    "\n",
    "            else:       #   \"STRT\n",
    "                start_row, start_col = random.choice(self.track.start_positions)\n",
    "                next_state = (start_row, start_col, 0, 0)\n",
    "        else:\n",
    "            next_state = (new_pos_row, new_pos_col, new_vel_row, new_vel_col)\n",
    "\n",
    "        # check if state is on the finish line\n",
    "        finished = self.track.is_finish((next_state[0], next_state[1]))\n",
    "        reward = 0.0 if finished else -1.0\n",
    "\n",
    "        if crashed and not finished:    # extra penalty for crashing\n",
    "            reward -= 10.0\n",
    "\n",
    "        done = finished\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def train(self):            #Q-learning update\n",
    "        print(\"Starting Q-learning training\")\n",
    "        for episode in range(self.number_episodes):\n",
    "            start_row, start_col = random.choice(self.track.start_positions)\n",
    "            state = (start_row, start_col, 0, 0)\n",
    "\n",
    "            for step in range(self.max_steps):      # iterate through environment\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.step_env(state, action)\n",
    "\n",
    "                q_val = self.get_q_val(state, action)\n",
    "\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    next_values = [self.get_q_val(next_state, a) for a in self.actions]\n",
    "                    best_next = max(next_values)\n",
    "                    target = reward + self.discount_factor * best_next\n",
    "\n",
    "                # update Q value\n",
    "                new_q_val = q_val + self.learning_rate * (target - q_val)\n",
    "                self.set_q_val(state, action, new_q_val)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            #progess checking of rewards\n",
    "            if (episode+1) % 100 == 0:\n",
    "                print(f\"Episode {episode+1}: {reward:.2f}\")\n",
    "\n",
    "        print(\"Q-learning training complete\")\n",
    "\n",
    "    def run_simulation(self, max_steps=1000):\n",
    "        start_row, start_col = random.choice(self.track.start_positions)\n",
    "        state = (start_row, start_col, 0, 0)\n",
    "        path = [state[:2]]\n",
    "        step_count = 0\n",
    "        success = False\n",
    "\n",
    "        while step_count < max_steps:\n",
    "            pos_row, pos_col, vel_row, vel_col = state\n",
    "\n",
    "            if self.track.is_finish((pos_row, pos_col)):\n",
    "                success = True\n",
    "                break\n",
    "\n",
    "            action = self.choose_greedy_action(state)\n",
    "            next_state, reward, done = self.step_env(state, action)\n",
    "            path.append(next_state[:2])\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "\n",
    "            if done:\n",
    "                success = True\n",
    "                break\n",
    "        return path, step_count, success\n",
    "\n",
    "def run_q_learning(group_id, track_name, crash_pos):\n",
    "    \"\"\"Run Q-Learning algorithm\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Running Q-Learning Algorithm\")\n",
    "    print(f\"Group: {group_id}\")\n",
    "    print(f\"Track: {track_name}\")\n",
    "    print(f\"Crash Scenario: {crash_pos}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load racetrack\n",
    "    try:\n",
    "        racetrack = Racetrack(track_name, crash_scenario=crash_pos)\n",
    "        print(f\"Track loaded: {racetrack.rows} x {racetrack.cols}\")\n",
    "        print(f\"Start positions: {len(racetrack.start_positions)}\")\n",
    "        print(f\"Finish positions: {len(racetrack.finish_positions)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Track file '{track_name}' not found.\")\n",
    "        return None\n",
    "\n",
    "    # Create and train Q-Learning racer\n",
    "    q_racer = QLearningRacer(racetrack)\n",
    "    q_racer.train()\n",
    "\n",
    "    # Evaluate learned policy\n",
    "    num_simulations = 10\n",
    "    successful_runs = 0\n",
    "    total_steps = 0\n",
    "    best_path = None\n",
    "    best_steps = float('inf')\n",
    "\n",
    "    print(f\"\\nRunning {num_simulations} simulations with learned policy...\")\n",
    "    for i in range(num_simulations):\n",
    "        path, steps, success = q_racer.run_simulation(max_steps=1500)\n",
    "\n",
    "        if success:\n",
    "            successful_runs += 1\n",
    "            total_steps += steps\n",
    "            if steps < best_steps:\n",
    "                best_steps = steps\n",
    "                best_path = path\n",
    "            print(f\"  Simulation {i+1}: Success in {steps} steps\")\n",
    "        else:\n",
    "            print(f\"  Simulation {i+1}: Failed to reach finish\")\n",
    "\n",
    "    print(\"\\nSimulation Results (Q-Learning):\")\n",
    "    print(f\"  Successful runs: {successful_runs}/{num_simulations}\")\n",
    "    if successful_runs > 0:\n",
    "        avg_steps = total_steps / successful_runs\n",
    "        print(f\"  Average steps: {avg_steps:.1f}\")\n",
    "        print(f\"  Best run: {best_steps} steps\")\n",
    "\n",
    "        # Plot best path\n",
    "        track_base = os.path.splitext(os.path.basename(track_name))[0]\n",
    "        q_racer.plot_path(best_path, group_id, \"QLrng\", track_base, crash_pos)\n",
    "    else:\n",
    "        print(\"  No successful runs to plot.\")\n",
    "\n",
    "    return q_racer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b500bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA\n",
    "\n",
    "class SARSARacer(BaseRacingAlgorithm):\n",
    "    def __init__(\n",
    "        self,\n",
    "        racetrack,\n",
    "        learning_rate=0.1,\n",
    "        discount_factor=0.95,\n",
    "        exploration_rate=0.1,\n",
    "        number_episodes=4000,\n",
    "        max_steps=1000,\n",
    "    ):\n",
    "        super().__init__(racetrack)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.number_episodes = number_episodes\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        self.actions = []\n",
    "        for acc_row in [-1, 0, 1]:\n",
    "            for acc_col in [-1, 0, 1]:\n",
    "                self.actions.append((acc_row, acc_col))\n",
    "        self.q_table = {}\n",
    "\n",
    "    def get_q_val(self, state, action):\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "\n",
    "    def set_q_val(self, state, action, val):\n",
    "        self.q_table[(state, action)] = val\n",
    "\n",
    "    def choose_greedy_action(self, state):\n",
    "        best_val = float(\"-inf\")\n",
    "        best_actions = []\n",
    "\n",
    "        for action in self.actions:\n",
    "            q_val = self.get_q_val(state, action)\n",
    "\n",
    "            if q_val > best_val:\n",
    "                best_val = q_val\n",
    "                best_actions = [action]\n",
    "            elif q_val == best_val:\n",
    "                best_actions.append(action)\n",
    "\n",
    "        if not best_actions:\n",
    "            return random.choice(self.actions)\n",
    "\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def choose_action(self, state, use_exploration=True):\n",
    "        if use_exploration and random.random() < self.exploration_rate:\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            return self.choose_greedy_action(state)\n",
    "\n",
    "    def step_env(self, state, action):\n",
    "        pos_row, pos_col, vel_row, vel_col = state\n",
    "        acc_row, acc_col = action\n",
    "\n",
    "        if random.random() < 0.8:\n",
    "            new_vel_row = min(5, max(-5, vel_row + acc_row))\n",
    "            new_vel_col = min(5, max(-5, vel_col + acc_col))\n",
    "        else:\n",
    "            new_vel_row = vel_row\n",
    "            new_vel_col = vel_col\n",
    "\n",
    "        new_pos_row = pos_row + new_vel_row\n",
    "        new_pos_col = pos_col + new_vel_col\n",
    "\n",
    "        crashed, crash_pos = self.track.check_crash((pos_row, pos_col), (new_pos_row, new_pos_col))\n",
    "\n",
    "        if crashed:\n",
    "            if self.track.crash_scenario == \"NRST\":\n",
    "                near_row, near_col = self.track.get_nearest_track_position(crash_pos)\n",
    "                next_state = (near_row, near_col, 0, 0)\n",
    "            else:\n",
    "                start_row, start_col = random.choice(self.track.start_positions)\n",
    "                next_state = (start_row, start_col, 0, 0)\n",
    "        else:\n",
    "            next_state = (new_pos_row, new_pos_col, new_vel_row, new_vel_col)\n",
    "\n",
    "        finished = self.track.is_finish((next_state[0], next_state[1]))\n",
    "        reward = 0.0 if finished else -1.0\n",
    "\n",
    "        if crashed and not finished:\n",
    "            reward -= 10.0\n",
    "\n",
    "        done = finished\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Starting SARSA training\")\n",
    "        for episode in range(self.number_episodes):\n",
    "            start_row, start_col = random.choice(self.track.start_positions)\n",
    "            state = (start_row, start_col, 0, 0)\n",
    "            \n",
    "            action = self.choose_action(state)\n",
    "\n",
    "            for step in range(self.max_steps):\n",
    "                next_state, reward, done = self.step_env(state, action)\n",
    "                \n",
    "                next_action = self.choose_action(next_state)\n",
    "\n",
    "                q_val = self.get_q_val(state, action)\n",
    "\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    next_q = self.get_q_val(next_state, next_action)\n",
    "                    target = reward + self.discount_factor * next_q\n",
    "\n",
    "                new_q_val = q_val + self.learning_rate * (target - q_val)\n",
    "                self.set_q_val(state, action, new_q_val)\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                print(f\"Episode {episode + 1}: {reward:.2f}\")\n",
    "\n",
    "        print(\"SARSA training complete\")\n",
    "\n",
    "    def run_simulation(self, max_steps=1000):\n",
    "        start_row, start_col = random.choice(self.track.start_positions)\n",
    "        state = (start_row, start_col, 0, 0)\n",
    "        path = [state[:2]]\n",
    "        step_count = 0\n",
    "        success = False\n",
    "\n",
    "        while step_count < max_steps:\n",
    "            pos_row, pos_col, vel_row, vel_col = state\n",
    "\n",
    "            if self.track.is_finish((pos_row, pos_col)):\n",
    "                success = True\n",
    "                break\n",
    "\n",
    "            action = self.choose_greedy_action(state)\n",
    "            next_state, reward, done = self.step_env(state, action)\n",
    "            path.append(next_state[:2])\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "\n",
    "            if done:\n",
    "                success = True\n",
    "                break\n",
    "        \n",
    "        return path, step_count, success\n",
    "\n",
    "\n",
    "def run_sarsa(group_id, track_name, crash_pos):\n",
    "    \"\"\"Run SARSA algorithm\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Running SARSA Algorithm\")\n",
    "    print(f\"Group: {group_id}\")\n",
    "    print(f\"Track: {track_name}\")\n",
    "    print(f\"Crash Scenario: {crash_pos}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load racetrack\n",
    "    try:\n",
    "        racetrack = Racetrack(track_name, crash_scenario=crash_pos)\n",
    "        print(f\"Track loaded: {racetrack.rows} x {racetrack.cols}\")\n",
    "        print(f\"Start positions: {len(racetrack.start_positions)}\")\n",
    "        print(f\"Finish positions: {len(racetrack.finish_positions)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Track file '{track_name}' not found.\")\n",
    "        return None\n",
    "\n",
    "    sarsa_racer = SARSARacer(racetrack)\n",
    "    sarsa_racer.train()\n",
    "\n",
    "    num_simulations = 10\n",
    "    successful_runs = 0\n",
    "    total_steps = 0\n",
    "    best_path = None\n",
    "    best_steps = float('inf')\n",
    "\n",
    "    print(f\"\\nRunning {num_simulations} simulations with learned policy...\")\n",
    "    for i in range(num_simulations):\n",
    "        path, steps, success = sarsa_racer.run_simulation(max_steps=1500)\n",
    "\n",
    "        if success:\n",
    "            successful_runs += 1\n",
    "            total_steps += steps\n",
    "            if steps < best_steps:\n",
    "                best_steps = steps\n",
    "                best_path = path\n",
    "            print(f\"  Simulation {i+1}: Success in {steps} steps\")\n",
    "        else:\n",
    "            print(f\"  Simulation {i+1}: Failed to reach finish\")\n",
    "\n",
    "    print(\"\\nSimulation Results (SARSA):\")\n",
    "    print(f\"  Successful runs: {successful_runs}/{num_simulations}\")\n",
    "    if successful_runs > 0:\n",
    "        avg_steps = total_steps / successful_runs\n",
    "        print(f\"  Average steps: {avg_steps:.1f}\")\n",
    "        print(f\"  Best run: {best_steps} steps\")\n",
    "\n",
    "        track_base = os.path.splitext(os.path.basename(track_name))[0]\n",
    "        sarsa_racer.plot_path(best_path, group_id, \"SARSA\", track_base, crash_pos)\n",
    "    else:\n",
    "        print(\"  No successful runs to plot.\")\n",
    "\n",
    "    return sarsa_racer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
